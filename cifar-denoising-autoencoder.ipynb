{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13821223,"sourceType":"datasetVersion","datasetId":8801793}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport os\n\n\nDATASET_PATH = '/kaggle/input/cifar-10-new/'\n\nprint(\"Files in dataset folder:\")\nprint(os.listdir(DATASET_PATH))\nprint()\n\n\ndef load_batch(batch_path):\n    \"\"\"\n    Load a single CIFAR-10 batch file\n    \n    Args:\n        batch_path: Full path to the batch file\n        \n    Returns:\n        data: numpy array of shape (10000, 3072) - flattened images\n        labels: list of length 10000 - class labels (0-9)\n    \"\"\"\n    with open(batch_path, 'rb') as f:\n        batch = pickle.load(f, encoding='bytes')\n    \n    data = batch[b'data']      # Shape: (10000, 3072)\n    labels = batch[b'labels']  # Shape: (10000,)\n    \n    return data, labels\n\n\n\nx_train_batches = []\ny_train_batches = []\n\nprint(\"Loading training batches...\")\nfor i in range(1, 6):  # i = 1, 2, 3, 4, 5\n    batch_filename = f'data_batch_{i}'\n    batch_path = os.path.join(DATASET_PATH, batch_filename)\n    \n    print(f\"  Loading {batch_filename}...\", end=' ')\n    data, labels = load_batch(batch_path)\n    \n    x_train_batches.append(data)\n    y_train_batches.append(labels)\n    \n    print(f\"‚úì Loaded {len(data)} images\")\n\nprint(f\"\\n Successfully loaded all 5 training batches!\")\nprint(f\"   Total batches: {len(x_train_batches)}\")\nprint(f\"   Each batch shape: {x_train_batches[0].shape}\")\nprint(f\"   Each batch has {len(y_train_batches[0])} labels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:15:40.591255Z","iopub.execute_input":"2025-11-22T15:15:40.591573Z","iopub.status.idle":"2025-11-22T15:15:43.088521Z","shell.execute_reply.started":"2025-11-22T15:15:40.591546Z","shell.execute_reply":"2025-11-22T15:15:43.087710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nConcatenating all training batches...\")\n\nx_train_full = np.concatenate(x_train_batches, axis=0)  # (50000, 3072)\n\ny_train_full = np.concatenate(y_train_batches, axis=0)  # (50000,)\n\nprint(f\" Concatenation complete!\")\nprint(f\"   Combined image data shape: {x_train_full.shape}  ‚Üê (50000 images, 3072 pixel values)\")\nprint(f\"   Combined labels shape: {y_train_full.shape}  ‚Üê (50000 class labels)\")\nprint()\n\n\nprint(\" Verification:\")\nprint(f\"   Total images: {len(x_train_full)}\")\nprint(f\"   Pixel values range: [{x_train_full.min()}, {x_train_full.max()}]  ‚Üê Still in [0, 255]\")\nprint(f\"   Unique class labels: {np.unique(y_train_full)}  ‚Üê Should be [0-9]\")\nprint(f\"   Label distribution:\")\nfor class_id in range(10):\n    count = np.sum(y_train_full == class_id)\n    print(f\"     Class {class_id}: {count} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:15:46.286231Z","iopub.execute_input":"2025-11-22T15:15:46.286912Z","iopub.status.idle":"2025-11-22T15:15:46.378619Z","shell.execute_reply.started":"2025-11-22T15:15:46.286885Z","shell.execute_reply":"2025-11-22T15:15:46.377943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nprint(\"Step 1: Reshaping images...\")\n\nx_train_full = x_train_full.reshape(-1, 3, 32, 32)  # (50000, 3, 32, 32)\n\nx_train_full = x_train_full.transpose(0, 2, 3, 1)   # (50000, 32, 32, 3)\n\nprint(f\" Reshaped to: {x_train_full.shape}\")\nprint(f\"   Format: (num_images, height, width, channels)\")\nprint()\n\nprint(\"Step 2: Normalizing pixel values...\")\n\nx_train_full = x_train_full.astype('float32') / 255.0\n\nprint(f\" Normalized!\")\nprint(f\"   Pixel value range: [{x_train_full.min():.3f}, {x_train_full.max():.3f}]\")\nprint(f\"   Data type: {x_train_full.dtype}\")\nprint()\n\n\nprint(\"Step 3: Splitting into train and validation sets...\")\n\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_full,\n    y_train_full,\n    test_size=0.2,          # 20% for validation\n    random_state=42,        # For reproducibility\n    stratify=y_train_full   # Maintain class balance\n)\n\nprint(f\" Split complete!\")\nprint(f\"   Training set: {x_train.shape} ({len(x_train)} images)\")\nprint(f\"   Validation set: {x_val.shape} ({len(x_val)} images)\")\nprint()\nprint(f\"   Training labels: {y_train.shape}\")\nprint(f\"   Validation labels: {y_val.shape}\")\nprint()\n\nprint(\" Class Distribution Verification:\")\nprint()\nprint(\"Training set:\")\nfor class_id in range(10):\n    count = np.sum(y_train == class_id)\n    print(f\"  Class {class_id}: {count} images\")\n\nprint()\nprint(\"Validation set:\")\nfor class_id in range(10):\n    count = np.sum(y_val == class_id)\n    print(f\"  Class {class_id}: {count} images\")\n\nprint()\nprint(\"=\" * 60)\nprint(\" PREPROCESSING COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"Ready for U-Net model:\")\nprint(f\"  x_train: {x_train.shape} - float32 in [0,1] range\")\nprint(f\"  x_val: {x_val.shape} - float32 in [0,1] range\")\nprint(f\"  Format: (N, H, W, C) = (samples, 32, 32, 3)\")\nprint(f\"  Next step: Create PyTorch Dataset with noise addition\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:15:48.950324Z","iopub.execute_input":"2025-11-22T15:15:48.951060Z","iopub.status.idle":"2025-11-22T15:15:50.194473Z","shell.execute_reply.started":"2025-11-22T15:15:48.951005Z","shell.execute_reply":"2025-11-22T15:15:50.193840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\n\nclass CIFAR10DenoisingDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for denoising autoencoder training.\n    \n    This Dataset:\n    - Stores clean images and labels\n    - Adds Gaussian noise on-the-fly when samples are accessed\n    - Converts image format from (H, W, C) to (C, H, W) for PyTorch Conv2d\n    - Returns (noisy_image, clean_image, label) tuples\n    \"\"\"\n    \n    def __init__(self, images, labels, noise_factor=0.3):\n        \"\"\"\n        Args:\n            images: numpy array of shape (N, 32, 32, 3) with values in [0, 1]\n            labels: numpy array of shape (N,) with class indices 0-9\n            noise_factor: standard deviation of Gaussian noise\n        \"\"\"\n        # Convert numpy arrays to PyTorch tensors\n        self.images = torch.from_numpy(images).float()  # (N, 32, 32, 3)\n        self.labels = torch.from_numpy(labels).long()    # (N,)\n        self.noise_factor = noise_factor\n    \n    def __len__(self):\n        \"\"\"Return the total number of images in the dataset\"\"\"\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Get a single sample with noise added.\n        Called by DataLoader for each sample in a batch.\n        \n        Args:\n            idx: index of the image to retrieve\n            \n        Returns:\n            noisy_img: image with Gaussian noise added, shape (3, 32, 32)\n            clean_img: original clean image, shape (3, 32, 32)\n            label: class label (0-9)\n        \"\"\"\n        clean_img = self.images[idx]  # (32, 32, 3)\n        label = self.labels[idx]\n        \n        noise = torch.randn_like(clean_img) * self.noise_factor\n        noisy_img = clean_img + noise\n        \n        noisy_img = torch.clamp(noisy_img, 0.0, 1.0)\n        \n        noisy_img = noisy_img.permute(2, 0, 1)  # (32, 32, 3) -> (3, 32, 32)\n        clean_img = clean_img.permute(2, 0, 1)  # (32, 32, 3) -> (3, 32, 32)\n        \n        return noisy_img, clean_img, label\n\nprint(\"Creating PyTorch Datasets...\")\n\n# Training dataset\ntrain_dataset = CIFAR10DenoisingDataset(\n    images=x_train,      # (40000, 32, 32, 3)\n    labels=y_train,      # (40000,)\n    noise_factor=0.3     # Adjust noise level as needed\n)\n\n# Validation dataset\nval_dataset = CIFAR10DenoisingDataset(\n    images=x_val,        # (10000, 32, 32, 3)\n    labels=y_val,        # (10000,)\n    noise_factor=0.3\n)\n\nprint(f\" Datasets created!\")\nprint(f\"   Training dataset: {len(train_dataset)} samples\")\nprint(f\"   Validation dataset: {len(val_dataset)} samples\")\nprint()\n\nprint(\" Verifying Dataset output format:\")\nnoisy_sample, clean_sample, label_sample = train_dataset[0]\nprint(f\"   Single sample shapes:\")\nprint(f\"   - Noisy image: {noisy_sample.shape}  ‚Üê (C, H, W) format\")\nprint(f\"   - Clean image: {clean_sample.shape}\")\nprint(f\"   - Label: {label_sample}  ‚Üê Class {label_sample}\")\nprint(f\"   - Noisy values range: [{noisy_sample.min():.3f}, {noisy_sample.max():.3f}]\")\nprint(f\"   - Clean values range: [{clean_sample.min():.3f}, {clean_sample.max():.3f}]\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:15:54.776366Z","iopub.execute_input":"2025-11-22T15:15:54.776772Z","iopub.status.idle":"2025-11-22T15:15:58.151915Z","shell.execute_reply.started":"2025-11-22T15:15:54.776747Z","shell.execute_reply":"2025-11-22T15:15:58.151129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nprint(\"\\nCreating PyTorch DataLoaders...\")\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,       # Number of images per batch\n    shuffle=True,         # Shuffle training data each epoch\n    num_workers=2,        # Parallel data loading (adjust based on your system)\n    pin_memory=True       # Faster data transfer to GPU\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=128,       # Same batch size\n    shuffle=False,        # Don't shuffle validation data\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"DataLoaders created!\")\nprint(f\"   Training batches per epoch: {len(train_loader)}\")\nprint(f\"   Validation batches per epoch: {len(val_loader)}\")\nprint()\n\n\nprint(\" Verifying DataLoader batch format:\")\n\nnoisy_batch, clean_batch, label_batch = next(iter(train_loader))\n\nprint(f\"   Batch shapes:\")\nprint(f\"   - Noisy batch: {noisy_batch.shape}  ‚Üê (batch, C, H, W)\")\nprint(f\"   - Clean batch: {clean_batch.shape}\")\nprint(f\"   - Label batch: {label_batch.shape}  ‚Üê (batch,)\")\nprint(f\"   - Noisy batch values: [{noisy_batch.min():.3f}, {noisy_batch.max():.3f}]\")\nprint(f\"   - Clean batch values: [{clean_batch.min():.3f}, {clean_batch.max():.3f}]\")\nprint()\n\nprint(\"=\" * 60)\nprint(\" DATASET AND DATALOADER SETUP COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"Ready for U-Net training:\")\nprint(f\"  - Train batches: {len(train_loader)} √ó {128} images\")\nprint(f\"  - Val batches: {len(val_loader)} √ó {128} images\")\nprint(f\"  - Format: (batch, 3, 32, 32) ‚Üê Perfect for your U-Net model\")\nprint(f\"  - Noise added on-the-fly during training\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:16:02.716616Z","iopub.execute_input":"2025-11-22T15:16:02.717319Z","iopub.status.idle":"2025-11-22T15:16:03.144926Z","shell.execute_reply.started":"2025-11-22T15:16:02.717295Z","shell.execute_reply":"2025-11-22T15:16:03.144070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNetDenoisingAutoencoder(nn.Module):\n    def __init__(self):\n        super(UNetDenoisingAutoencoder, self).__init__()\n\n        initial_filters = 32  \n\n        def conv_block(in_channels, out_channels):\n            \"\"\"\n            Standard convolution block with two Conv2d layers + ReLU\n            Maintains spatial dimensions (padding=1 with kernel_size=3)\n            \"\"\"\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),  # Added BatchNorm for stability\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n        \n        # Encoder Block 1\n        self.enc1 = conv_block(3, initial_filters)     # Input: (3, 32, 32) ‚Üí Output: (32, 32, 32)\n        self.pool1 = nn.MaxPool2d(2)                   # Output: (32, 16, 16)\n\n        # Encoder Block 2\n        self.enc2 = conv_block(initial_filters, initial_filters * 2)  # Output: (64, 16, 16)\n        self.pool2 = nn.MaxPool2d(2)                   # Output: (64, 8, 8)\n\n        \n        self.bottleneck = conv_block(initial_filters * 2, initial_filters * 4)  # Output: (128, 8, 8)\n\n        \n        # Decoder Block 2\n        self.upconv2 = nn.ConvTranspose2d(initial_filters * 4, initial_filters * 2, \n                                          kernel_size=2, stride=2)  # Output: (64, 16, 16)\n        # After concatenation with enc2: (64 + 64 = 128, 16, 16)\n        self.dec2 = conv_block(initial_filters * 2 + initial_filters * 2, initial_filters * 2)  # Output: (64, 16, 16)\n\n        # Decoder Block 1\n        self.upconv1 = nn.ConvTranspose2d(initial_filters * 2, initial_filters, \n                                          kernel_size=2, stride=2)  # Output: (32, 32, 32)\n        # After concatenation with enc1: (32 + 32 = 64, 32, 32)\n        self.dec1 = conv_block(initial_filters + initial_filters, initial_filters)  # Output: (32, 32, 32)\n\n    \n        self.final_conv = nn.Conv2d(initial_filters, 3, kernel_size=1)  # Output: (3, 32, 32)\n        self.sigmoid = nn.Sigmoid()  # Output values in [0, 1]\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass with skip connections\n        \n        Input: x of shape (batch, 3, 32, 32)\n        Output: denoised image of shape (batch, 3, 32, 32)\n        \"\"\"\n    \n        e1 = self.enc1(x)      # (batch, 32, 32, 32)\n        p1 = self.pool1(e1)    # (batch, 32, 16, 16)\n\n        e2 = self.enc2(p1)     # (batch, 64, 16, 16)\n        p2 = self.pool2(e2)    # (batch, 64, 8, 8)\n\n        bottleneck = self.bottleneck(p2)  # (batch, 128, 8, 8)\n        \n        # Upsample and concatenate with enc2\n        d2 = self.upconv2(bottleneck)      # (batch, 64, 16, 16)\n        d2 = torch.cat((d2, e2), dim=1)    # (batch, 128, 16, 16) - Skip connection\n        d2 = self.dec2(d2)                 # (batch, 64, 16, 16)\n\n        # Upsample and concatenate with enc1\n        d1 = self.upconv1(d2)              # (batch, 32, 32, 32)\n        d1 = torch.cat((d1, e1), dim=1)    # (batch, 64, 32, 32) - Skip connection\n        d1 = self.dec1(d1)                 # (batch, 32, 32, 32)\n\n        output = self.final_conv(d1)       # (batch, 3, 32, 32)\n        output = self.sigmoid(output)      # Values in [0, 1]\n\n        return output\n\nif __name__ == \"__main__\":\n    model = UNetDenoisingAutoencoder()\n    \n    dummy_input = torch.randn(4, 3, 32, 32)\n    \n    # Forward pass\n    output = model(dummy_input)\n    \n    print(\"=\" * 60)\n    print(\"U-Net Architecture for CIFAR-10 (32√ó32)\")\n    print(\"=\" * 60)\n    print(f\"Input shape:  {dummy_input.shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n    print(\"\\n Model is ready for CIFAR-10 denoising!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:16:07.952688Z","iopub.execute_input":"2025-11-22T15:16:07.953478Z","iopub.status.idle":"2025-11-22T15:16:08.104207Z","shell.execute_reply.started":"2025-11-22T15:16:07.953445Z","shell.execute_reply":"2025-11-22T15:16:08.103357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import vgg16\n\n\nclass PerceptualLoss(nn.Module):\n    \"\"\"\n    Perceptual Loss using VGG16 pre-trained features.\n    \n    Compares high-level features extracted from a pre-trained VGG16 network\n    instead of raw pixel values. This better matches human perception of\n    image similarity.\n    \"\"\"\n    \n    def __init__(self, device='cuda'):\n        super(PerceptualLoss, self).__init__()\n        \n        # Load pre-trained VGG16 model\n        vgg = vgg16(pretrained=True).features.to(device)\n        \n        self.feature_extractor = nn.Sequential(*list(vgg.children())[:23]).eval()\n        \n        for param in self.feature_extractor.parameters():\n            param.requires_grad = False\n        \n        self.mse_loss = nn.MSELoss()\n        \n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n    \n    def normalize(self, x):\n        \"\"\"Normalize images from [0,1] to ImageNet normalization\"\"\"\n        mean = self.mean.to(x.device)\n        std = self.std.to(x.device)\n        return (x - mean) / std\n    \n    def forward(self, output, target):\n        \"\"\"\n        Calculate perceptual loss between output and target images.\n        \n        Args:\n            output: Denoised images (batch, 3, 32, 32) in [0, 1]\n            target: Clean images (batch, 3, 32, 32) in [0, 1]\n            \n        Returns:\n            Perceptual loss value\n        \"\"\"\n        output_normalized = self.normalize(output)\n        target_normalized = self.normalize(target)\n        \n        output_features = self.feature_extractor(output_normalized)\n        target_features = self.feature_extractor(target_normalized)\n        \n        perceptual_loss = self.mse_loss(output_features, target_features)\n        \n        return perceptual_loss\n\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Initialize model\nmodel = UNetDenoisingAutoencoder().to(device)\n\n# Define Loss Function (Perceptual Loss only)\ncriterion = PerceptualLoss(device=device)\n\nprint(\" Loss function defined: Perceptual Loss\")\nprint(f\"   - Based on VGG16 pre-trained features\")\nprint(f\"   - Focuses on visual quality and high-level features\")\n\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=0.001,              # Learning rate\n    betas=(0.9, 0.999),    # Beta parameters for Adam\n    eps=1e-8,              # Epsilon for numerical stability\n    weight_decay=1e-5      # L2 regularization\n)\n\nprint(\" Optimizer defined: Adam\")\nprint(f\"   - Learning rate: 0.001\")\nprint(f\"   - Weight decay: 1e-5\")\nprint(f\"   - Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=5,\n    verbose=True\n)\n\nprint(\"Learning rate scheduler defined: ReduceLROnPlateau\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOSS FUNCTION AND OPTIMIZER SETUP COMPLETE!\")\nprint(\"=\"*60)\nprint(\"Ready for training loop.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:16:38.602545Z","iopub.execute_input":"2025-11-22T15:16:38.603232Z","iopub.status.idle":"2025-11-22T15:16:40.260520Z","shell.execute_reply.started":"2025-11-22T15:16:38.603205Z","shell.execute_reply":"2025-11-22T15:16:40.259907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport matplotlib.pyplot as plt\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"VERIFYING DATALOADERS\")\nprint(\"=\"*70)\n\ntry:\n    print(f\" train_loader found: {len(train_loader)} batches\")\n    print(f\" val_loader found: {len(val_loader)} batches\")\nexcept NameError:\n    raise NameError(\n        \"train_loader and/or val_loader not found!\\n\"\n        \"Please run the DataLoader creation code first.\"\n    )\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING U-NET DENOISING TRAINING ON CIFAR-10 WITH MULTI-GPU DataParallel\")\nprint(\"=\"*70)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nüìç Using device: {device}\")\n\nmodel = UNetDenoisingAutoencoder().to(device)\n\nif torch.cuda.device_count() > 1:\n    print(f\"‚öôÔ∏è {torch.cuda.device_count()} GPUs detected. Using DataParallel.\")\n    model = nn.DataParallel(model)\n\nprint(f\" Model loaded with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters\")\n\ncriterion = PerceptualLoss(device=device)\nprint(\" Loss function: Perceptual Loss (VGG16-based)\")\n\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    eps=1e-8,\n    weight_decay=1e-5\n)\nprint(\" Optimizer: Adam (lr=0.001)\")\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=5,\n    verbose=True\n)\nprint(\" Scheduler: ReduceLROnPlateau (reduces LR if validation loss plateaus)\")\n\nnum_epochs = 50  \nprint(f\"\\n Training configuration:\")\nprint(f\"   - Number of epochs: {num_epochs}\")\nprint(f\"   - Training batches: {len(train_loader)}\")\nprint(f\"   - Validation batches: {len(val_loader)}\")\nprint(f\"   - Batch size: {train_loader.batch_size}\")\n\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING STARTED\")\nprint(\"=\"*70 + \"\\n\")\n\nstart_time = time.time()\n\n\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n    \n    \n    model.train()  # Set model to training mode\n    train_loss = 0.0\n    \n    for batch_idx, (noisy_batch, clean_batch, labels) in enumerate(train_loader):\n        noisy_batch = noisy_batch.to(device)\n        clean_batch = clean_batch.to(device)\n        \n        outputs = model(noisy_batch)\n        \n        loss = criterion(outputs, clean_batch)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        \n        if (batch_idx + 1) % 50 == 0:\n            print(f\"   Epoch [{epoch+1}/{num_epochs}] \"\n                  f\"Batch [{batch_idx+1}/{len(train_loader)}] \"\n                  f\"Loss: {loss.item():.6f}\")\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    \n    model.eval()  \n    val_loss = 0.0\n    \n    with torch.no_grad():  \n        for noisy_batch, clean_batch, labels in val_loader:\n            noisy_batch = noisy_batch.to(device)\n            clean_batch = clean_batch.to(device)\n            \n            outputs = model(noisy_batch)\n            \n            loss = criterion(outputs, clean_batch)\n            \n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n    \n    scheduler.step(avg_val_loss)\n    \n    epoch_time = time.time() - epoch_start_time\n    \n    print(f\"\\n{'='*70}\")\n    print(f\" EPOCH {epoch+1}/{num_epochs} SUMMARY\")\n    print(f\"{'='*70}\")\n    print(f\"   Training Perceptual Loss:   {avg_train_loss:.6f}\")\n    print(f\"   Validation Perceptual Loss: {avg_val_loss:.6f}\")\n    print(f\"   Epoch Time: {epoch_time:.2f}s\")\n    print(f\"   Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n        }, 'best_unet_denoising_model.pth')\n        print(f\"   ‚úÖ Best model saved! (Val Loss: {best_val_loss:.6f})\")\n    \n    print(f\"{'='*70}\\n\")\n\n\ntotal_time = time.time() - start_time\nprint(\"\\n\" + \"=\"*70)\nprint(\" TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(f\"   Total Training Time: {total_time/60:.2f} minutes\")\nprint(f\"   Best Validation Loss: {best_val_loss:.6f}\")\nprint(f\"   Final Training Loss: {train_losses[-1]:.6f}\")\nprint(f\"   Final Validation Loss: {val_losses[-1]:.6f}\")\n\ntorch.save({\n    'epoch': num_epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'train_loss': train_losses[-1],\n    'val_loss': val_losses[-1],\n    'train_loss_history': train_losses,\n    'val_loss_history': val_losses,\n}, '/kaggle/working/final_unet_denoising_model.pth')\n\nprint(\"\\n Models saved:\")\nprint(\"   - best_unet_denoising_model.pth (lowest validation loss)\")\nprint(\"   - final_unet_denoising_model.pth (last epoch)\")\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o', markersize=4)\nplt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s', markersize=4)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Perceptual Loss', fontsize=12)\nplt.title('Training and Validation Loss Over Epochs', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_loss_plot.png', dpi=300)\nprint(\"\\n Training loss plot saved: training_loss_plot.png\")\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\" ALL DONE! Ready for testing phase.\")\nprint(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T15:16:44.059565Z","iopub.execute_input":"2025-11-22T15:16:44.060144Z","iopub.status.idle":"2025-11-22T15:43:59.764355Z","shell.execute_reply.started":"2025-11-22T15:16:44.060112Z","shell.execute_reply":"2025-11-22T15:43:59.763567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test_batches = []\ny_test_batches = []\n\nprint(\"Loading testing batch...\")\n\nbatch_filename = f'test_batch'\nbatch_path = os.path.join(DATASET_PATH, batch_filename)\n    \nprint(f\"  Loading {batch_filename}...\", end=' ')\ndata, labels = load_batch(batch_path)\n    \nx_test_batches.append(data)\ny_test_batches.append(labels)\n    \nprint(f\"‚úì Loaded {len(data)} images\")\n\nprint(f\"\\n Successfully loaded all 5 training batches!\")\nprint(f\"   Total batches: {len(x_test_batches)}\")\nprint(f\"   Each batch shape: {x_test_batches[0].shape}\")\nprint(f\"   Each batch has {len(y_test_batches[0])} labels\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:49:45.320816Z","iopub.execute_input":"2025-11-22T16:49:45.321757Z","iopub.status.idle":"2025-11-22T16:49:45.415198Z","shell.execute_reply.started":"2025-11-22T16:49:45.321715Z","shell.execute_reply":"2025-11-22T16:49:45.414582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Step 1: Reshaping images...\")\n\nx_test_batches = np.array(x_test_batches)\ny_test_batches = np.array(y_test_batches)\n\nx_test_batches = x_test_batches.reshape(-1, 3, 32, 32)  # (50000, 3, 32, 32)\n\nx_test_batches = x_test_batches.transpose(0, 2, 3, 1)   # (50000, 32, 32, 3)\n\nprint(f\"Reshaped to: {x_test_batches.shape}\")\nprint(f\"   Format: (num_images, height, width, channels)\")\nprint()\n\n\nprint(\"Step 2: Normalizing pixel values...\")\n\nx_test_batches = x_test_batches.astype('float32') / 255.0\n\n\ny_test_batches = np.array(y_test_batches).flatten()\n\nprint(f\" Normalized!\")\nprint(f\"   Pixel value range: [{x_test_batches.min():.3f}, {x_test_batches.max():.3f}]\")\nprint(f\"   Data type: {x_test_batches.dtype}\")\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:49:52.308407Z","iopub.execute_input":"2025-11-22T16:49:52.309049Z","iopub.status.idle":"2025-11-22T16:49:52.580618Z","shell.execute_reply.started":"2025-11-22T16:49:52.309025Z","shell.execute_reply":"2025-11-22T16:49:52.579765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CIFAR10TestDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for denoising autoencoder testing.\n    Returns clean images (as tensors in C, H, W) and their class labels.\n    \"\"\"\n    def __init__(self, images, labels):\n        \"\"\"\n        images: numpy array shape (N, 32, 32, 3), values in [0, 1]\n        labels: numpy array shape (N,)\n        \"\"\"\n        self.images = torch.from_numpy(images).float()    # [N, 32, 32, 3]\n        self.labels = torch.from_numpy(labels).long()      # [N]\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        clean_img = self.images[idx]                      # (32, 32, 3)\n        label = self.labels[idx]\n        # Convert to (C, H, W) for PyTorch models\n        clean_img = clean_img.permute(2, 0, 1)            # (3, 32, 32)\n        return clean_img, label\n\ntest_dataset = CIFAR10TestDataset(\n    images=x_test_batches,        # (10000, 32, 32, 3)\n    labels=y_test_batches,        # (10000,)\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:49:55.897357Z","iopub.execute_input":"2025-11-22T16:49:55.897655Z","iopub.status.idle":"2025-11-22T16:49:55.903617Z","shell.execute_reply.started":"2025-11-22T16:49:55.897632Z","shell.execute_reply":"2025-11-22T16:49:55.902844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = DataLoader(\n    test_dataset,\n    batch_size=128,       \n    shuffle=False,       \n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\" DataLoaders created!\")\nprint(f\"   Testing batches per epoch: {len(test_loader)}\")\nprint()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:49:58.853958Z","iopub.execute_input":"2025-11-22T16:49:58.854616Z","iopub.status.idle":"2025-11-22T16:49:58.859856Z","shell.execute_reply.started":"2025-11-22T16:49:58.854567Z","shell.execute_reply":"2025-11-22T16:49:58.859153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport os\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nNOISE_STD = 0.1  # Gaussian noise std used in training\nNUM_CLASSES = 10 # CIFAR-10 classes\n\nmodel_checkpoint_path = os.path.join('/kaggle/working', 'best_unet_denoising_model.pth')\n\nmodel = UNetDenoisingAutoencoder().to(device)\nif torch.cuda.device_count() > 1:\n    print(f\"Multi-GPU detected: {torch.cuda.device_count()} GPUs -- using DataParallel.\")\n    model = nn.DataParallel(model)\nelse:\n    print(f\"Single GPU or CPU detected -- using standard inference.\")\n\ncheckpoint = torch.load(model_checkpoint_path, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()  # Set to evaluation mode\n\nperceptual_criterion = PerceptualLoss(device=device)\n\nprint(\"Starting UNet denoising testing...\")\nclass_loss_counters = defaultdict(list)\n\nwith torch.no_grad():\n    for batch_idx, (images, labels) in enumerate(test_loader):\n        images = images.to(device)      # [B, 3, 32, 32] in [0, 1]\n        labels = labels.to(device)\n\n        noisy_images = images + (NOISE_STD * torch.randn_like(images))\n        noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n\n        denoised_images = model(noisy_images)\n\n        for i in range(denoised_images.size(0)):\n            pred = denoised_images[i].unsqueeze(0)   # shape [1, 3, 32, 32]\n            target = images[i].unsqueeze(0)\n            class_idx = labels[i].item()\n            loss_val = perceptual_criterion(pred, target).item()\n            class_loss_counters[class_idx].append(loss_val)\n\n        if (batch_idx + 1) % 20 == 0:\n            print(f\"Tested batch {batch_idx + 1}/{len(test_loader)}\")\n\nprint(\"Aggregating and plotting per-class test loss...\")\n\nmean_loss_per_class = []\nfor class_idx in range(NUM_CLASSES):\n    losses = class_loss_counters[class_idx]\n    if losses:\n        mean_loss = np.mean(losses)\n    else:\n        mean_loss = float('nan')\n    mean_loss_per_class.append(mean_loss)\n\nplt.figure(figsize=(9, 6))\nplt.bar(np.arange(NUM_CLASSES), mean_loss_per_class, color='royalblue', edgecolor='k', alpha=0.8)\nplt.xticks(np.arange(NUM_CLASSES), [str(i) for i in range(NUM_CLASSES)], fontsize=12)\nplt.xlabel('Class label', fontsize=14)\nplt.ylabel('Average Perceptual Loss', fontsize=14)\nplt.title('UNet Denoising Autoencoder\\nTest Perceptual Loss by Class', fontsize=16, fontweight='bold')\nplt.grid(axis='y', linestyle=':', alpha=0.4)\nplt.tight_layout()\nplt.savefig('/kaggle/working/test_perceptual_loss_per_class.png', dpi=300)\nprint(\"\\n Per-class perceptual loss plot saved: /kaggle/working/test_perceptual_loss_per_class.png\")\nplt.show()\n\nfor class_idx, mean_loss in enumerate(mean_loss_per_class):\n    print(f\"Class {class_idx}: Mean perceptual loss = {mean_loss:.5f}\")\n\nprint(\"\\n Testing complete and results saved.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:50:40.002147Z","iopub.execute_input":"2025-11-22T16:50:40.002772Z","iopub.status.idle":"2025-11-22T16:51:08.825547Z","shell.execute_reply.started":"2025-11-22T16:50:40.002747Z","shell.execute_reply":"2025-11-22T16:51:08.824641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(\n    np.arange(NUM_CLASSES), \n    mean_loss_per_class, \n    marker='o', \n    markersize=5, \n    color='royalblue', \n    linewidth=2, \n    label='Test Perceptual Loss'\n)\nplt.xlabel('Class Label', fontsize=12)\nplt.ylabel('Average Perceptual Loss', fontsize=12)\nplt.title('Test Perceptual Loss by Class (UNet Denoising Autoencoder)', fontsize=16, fontweight='bold')\nplt.xticks(np.arange(NUM_CLASSES), [str(i) for i in range(NUM_CLASSES)], fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=11)\nplt.tight_layout()\nplt.savefig('/kaggle/working/test_perceptual_loss_per_class_lineplot.png', dpi=300)\nprint(\"\\n Per-class loss line plot saved: /kaggle/working/test_perceptual_loss_per_class_lineplot.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:54:35.200973Z","iopub.execute_input":"2025-11-22T16:54:35.201565Z","iopub.status.idle":"2025-11-22T16:54:35.832571Z","shell.execute_reply.started":"2025-11-22T16:54:35.201534Z","shell.execute_reply":"2025-11-22T16:54:35.832027Z"}},"outputs":[],"execution_count":null}]}